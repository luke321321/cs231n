{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tnrange, tqdm_notebook, tqdm\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (48000, 40, 40, 3)\n",
      "Train labels shape:  (48000,) int32\n",
      "Validation data shape:  (2000, 40, 40, 3)\n",
      "Validation labels shape:  (2000,)\n",
      "Test data shape:  (10000, 40, 40, 3)\n",
      "Test labels shape:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "def load_cifar10(num_training=48000, num_validation=2000, num_test=10000):\n",
    "    \"\"\"\n",
    "    Fetch the CIFAR-10 dataset from the web and perform preprocessing to prepare\n",
    "    it.  We pad the training data here for the data augmentation\n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 dataset and use appropriate data types and shapes\n",
    "    cifar10 = tf.keras.datasets.cifar10.load_data()\n",
    "    (X_train, y_train), (X_test, y_test) = cifar10\n",
    "    X_train = np.asarray(X_train, dtype=np.float32)\n",
    "    y_train = np.asarray(y_train, dtype=np.int32).flatten()\n",
    "    X_test = np.asarray(X_test, dtype=np.float32)\n",
    "    y_test = np.asarray(y_test, dtype=np.int32).flatten()\n",
    "\n",
    "    # Subsample the data\n",
    "    mask = range(num_training, num_training + num_validation)\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = range(num_training)\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = range(num_test)\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "\n",
    "    # Normalize the data: subtract the mean pixel and divide by std\n",
    "    mean_pixel = X_train.mean(axis=(0, 1, 2), keepdims=True)\n",
    "    std_pixel = X_train.std(axis=(0, 1, 2), keepdims=True)\n",
    "    X_train = (X_train - mean_pixel) / std_pixel\n",
    "    X_val = (X_val - mean_pixel) / std_pixel\n",
    "    X_test = (X_test - mean_pixel) / std_pixel\n",
    "    \n",
    "    #Pad the data by 4 on height and width\n",
    "    paddings = [[0, 0,], [4, 4], [4, 4], [0, 0]]\n",
    "    X_train = np.pad(X_train, paddings, 'constant')\n",
    "    X_val = np.pad(X_val, paddings, 'constant')\n",
    "    X_test = np.pad(X_test, paddings, 'constant')\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_cifar10()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape, y_train.dtype)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a basic dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    def __init__(self, X, y, batch_size, shuffle=False):\n",
    "        \"\"\"\n",
    "        Construct a Dataset object to iterate over data X and labels y\n",
    "        \n",
    "        Inputs:\n",
    "        - X: Numpy array of data, of any shape\n",
    "        - y: Numpy array of labels, of any shape but with y.shape[0] == X.shape[0]\n",
    "        - batch_size: Integer giving number of elements per minibatch\n",
    "        - shuffle: (optional) Boolean, whether to shuffle the data on each epoch\n",
    "        \"\"\"\n",
    "        assert X.shape[0] == y.shape[0], 'Got different numbers of data and labels'\n",
    "        self.X, self.y = X, y\n",
    "        self.batch_size, self.shuffle = batch_size, shuffle\n",
    "\n",
    "    def __iter__(self):\n",
    "        N, B = self.X.shape[0], self.batch_size\n",
    "        idxs = np.arange(N)\n",
    "        X = self.X\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(idxs)\n",
    "            \n",
    "        return iter((X[i:i+B], self.y[i:i+B]) for i in range(0, N, B))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y) // self.batch_size\n",
    "    \n",
    "\n",
    "batch_size = 64\n",
    "train_dset = Dataset(X_train, y_train, batch_size=batch_size, shuffle=True)\n",
    "val_dset = Dataset(X_val, y_val, batch_size=batch_size, shuffle=False)\n",
    "test_dset = Dataset(X_test, y_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create our neutral network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "initializer = tf.variance_scaling_initializer()\n",
    "\n",
    "#Helper layer function\n",
    "def batch_norm_relu_conv2d_drop(inputs, filters, is_training, dropout, stride=1, reg=1e-4):\n",
    "    inputs = batch_norm_relu(inputs, is_training)\n",
    "    inputs = conv2d_drop(inputs,filters, is_training, dropout, stride=stride, reg=reg)\n",
    "    return inputs\n",
    "\n",
    "def batch_norm_relu(inputs, is_training):\n",
    "    inputs = tf.layers.batch_normalization(inputs, training=is_training)\n",
    "    return tf.nn.relu(inputs)\n",
    "\n",
    "def conv2d_drop(inputs, filters, is_training, dropout, stride=1, reg=1e-4):\n",
    "    inputs = tf.layers.conv2d(inputs, filters, 3, strides=stride, padding=\"same\", kernel_initializer=initializer,\n",
    "                           kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=reg),\n",
    "                             bias_regularizer= tf.contrib.layers.l2_regularizer(scale=reg))\n",
    "    if dropout is not None:\n",
    "        inputs = tf.nn.dropout(inputs, keep_prob=dropout)\n",
    "    return inputs\n",
    "\n",
    "#Resnet unit\n",
    "def ResNet_unit(inputs, filters, is_training, dropout, i, j, subsample=False, reg=1e-4, final_unit=False):\n",
    "    with tf.variable_scope(f\"conv{i+2}_{j+1}\"):\n",
    "        shortcut = inputs\n",
    "        stride = 2 if subsample else 1\n",
    "        \n",
    "        #for the first unit batch_norm_relu before splitting into two paths\n",
    "        if i == 0 and j == 0:\n",
    "            inputs = batch_norm_relu(inputs, is_training)\n",
    "            shortcut = inputs\n",
    "            inputs = conv2d_drop(inputs, filters, is_training, dropout, stride=stride, reg=reg)\n",
    "        else:\n",
    "            inputs = batch_norm_relu_conv2d_drop(inputs, filters, is_training, dropout, stride=stride, reg=reg)\n",
    "        inputs = batch_norm_relu_conv2d_drop(inputs, filters, is_training, dropout, reg=reg)\n",
    "        \n",
    "        if subsample:\n",
    "            paddings = tf.constant([[0,0], [0,0], [0,0], [0, filters // 2]])\n",
    "            shortcut = tf.pad(shortcut, paddings)\n",
    "            #reduce image height and width by striding as in resnet paper\n",
    "            shortcut = shortcut[:, ::2, ::2, :]\n",
    "            \n",
    "        inputs = shortcut + inputs\n",
    "        \n",
    "        #Final activation\n",
    "        if final_unit:\n",
    "            inputs = batch_norm_relu(inputs, is_training)\n",
    "        \n",
    "        return inputs\n",
    "    \n",
    "def model_ResNetv2(inputs, is_training, total_layers=20, dropout=None, num_classes=10, reg=1e-4):\n",
    "    num_layers = (total_layers - 2) // 6\n",
    "    filters = [16, 32, 64]\n",
    "    if dropout == 1: dropout = None\n",
    "    \n",
    "    with tf.variable_scope(\"data_augmentation\"):\n",
    "        if is_training == True:\n",
    "            inputs = tf.image.random_flip_left_right(inputs)\n",
    "\n",
    "            #random crop back to 32x32 (training data padded when preprocessed)\n",
    "            inputs = tf.random_crop(inputs, [inputs.shape[0], 32, 32, 3])\n",
    "        else:\n",
    "            #central crop back to 32x32\n",
    "            inputs = inputs[:, 4:36, 4:36, :]\n",
    "    \n",
    "    #first do a single convolution ResNet_unit with no addition\n",
    "    with tf.variable_scope(\"conv1\"):\n",
    "        inputs = conv2d_drop(inputs, filters[0], is_training, dropout, reg=reg)\n",
    "    \n",
    "    #now some ResNet units\n",
    "    for i in range(3):\n",
    "        for j in range(num_layers):\n",
    "            #don't subsample on first go round\n",
    "            subsample = i > 0 and j == 0\n",
    "            final = i == 2 and j == num_layers-1\n",
    "            inputs = ResNet_unit(inputs, filters[i], is_training,\n",
    "                                 dropout, i, j, subsample=subsample, reg=reg, final_unit=final)\n",
    "             \n",
    "    #Global average pooling, 10 way FC layer and then output to scores.\n",
    "    #Global average pooling is same as doing reduce_mean\n",
    "    inputs = tf.reduce_mean(inputs, axis=[1,2])\n",
    "    inputs = tf.layers.flatten(inputs)\n",
    "    scores = tf.layers.dense(inputs, num_classes, kernel_initializer=initializer,\n",
    "                            kernel_regularizer=tf.contrib.layers.l2_regularizer(scale=reg),\n",
    "                             bias_regularizer= tf.contrib.layers.l2_regularizer(scale=reg))\n",
    "    return scores "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A small test to check that our neutral network works correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 10)\n"
     ]
    }
   ],
   "source": [
    "def test_model_ResNet_fc():\n",
    "    \"\"\" A small unit test for model_ResNetv2 above. \"\"\"\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    x = tf.zeros((50, 40, 40, 3))\n",
    "    scores = model_ResNetv2(x, 1)\n",
    "        \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        scores_np = sess.run(scores)\n",
    "        print(scores_np.shape)\n",
    "        \n",
    "test_model_ResNet_fc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_acc_tb(sess, dset, x, scores, is_training, FLAG_print=True):\n",
    "    \"\"\"\n",
    "    Check accuracy on a classification model.\n",
    "    \n",
    "    Inputs:\n",
    "    - sess: A TensorFlow Session that will be used to run the graph\n",
    "    - dset: A Dataset object on which to check accuracy\n",
    "    - x: A TensorFlow placeholder Tensor where input images should be fed\n",
    "    - scores: A TensorFlow Tensor representing the scores output from the\n",
    "      model; this is the Tensor we will ask TensorFlow to evaluate.\n",
    "      \n",
    "    Returns: Accuracy of the model\n",
    "    \"\"\"\n",
    "    num_correct, num_samples = 0, 0\n",
    "    with tf.name_scope('accuracy'):\n",
    "        for x_batch, y_batch in dset:\n",
    "            feed_dict = {x: x_batch, is_training: 0}\n",
    "            scores_np = sess.run(scores, feed_dict=feed_dict)\n",
    "            y_pred = scores_np.argmax(axis=1)\n",
    "            num_samples += x_batch.shape[0]\n",
    "            num_correct += (y_pred == y_batch).sum()\n",
    "        acc = float(num_correct) / num_samples\n",
    "        if FLAG_print == True:\n",
    "            print('Got %d / %d correct (%.2f%%)' % (num_correct, num_samples, 100 * acc))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_acc_train(sess, x_batch, y_batch, x, scores, is_training, FLAG_print=True):\n",
    "    \"\"\"\n",
    "    Check accuracy on a classification model from a batch of data.\n",
    "    \n",
    "    Inputs:\n",
    "    - sess: A TensorFlow Session that will be used to run the graph\n",
    "    - dset: A Dataset object on which to check accuracy\n",
    "    - x: A TensorFlow placeholder Tensor where input images should be fed\n",
    "    - scores: A TensorFlow Tensor representing the scores output from the\n",
    "      model; this is the Tensor we will ask TensorFlow to evaluate.\n",
    "      \n",
    "    Returns: Accuracy of the model\n",
    "    \"\"\"\n",
    "    num_correct, num_samples = 0, 0\n",
    "    with tf.name_scope('accuracy'):\n",
    "        feed_dict = {x: x_batch, is_training: 0}\n",
    "        scores_np = sess.run(scores, feed_dict=feed_dict)\n",
    "        y_pred = scores_np.argmax(axis=1)\n",
    "        acc = float((y_pred == y_batch).sum()) / x_batch.shape[0]\n",
    "        if FLAG_print == True:\n",
    "            print('Got %d / %d correct (%.2f%%)' % (num_correct, num_samples, 100 * acc))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_part5(model_init_fn, optimizer_init_fn, num_epochs=1, decay_at=None, decay_to=None,\n",
    "                experiment_name=\"\", restore=False, epoch_num=1):\n",
    "    \"\"\"\n",
    "    Simple training loop for use with models defined using tf.keras. It trains\n",
    "    a model for one epoch on the CIFAR-10 training set and periodically checks\n",
    "    accuracy on the CIFAR-10 validation set.\n",
    "    \n",
    "    Inputs:\n",
    "    - model_init_fn: A function that takes no parameters; when called it\n",
    "      constructs the model we want to train: model = model_init_fn()\n",
    "    - optimizer_init_fn: A function which takes no parameters; when called it\n",
    "      constructs the Optimizer object we will use to optimize the model:\n",
    "      optimizer = optimizer_init_fn()\n",
    "    - num_epochs: The number of epochs to train for\n",
    "    - decay_at: Epochs to decay the learning rate\n",
    "    - decay_to: The learning rate to decay to at decay_at epochs\n",
    "    \n",
    "    Returns: Nothing, but prints progress during training\n",
    "    \"\"\"\n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    # declare placeholders\n",
    "    x = tf.placeholder(tf.float32, [None, 40, 40, 3])\n",
    "    y = tf.placeholder(tf.int32, [None])\n",
    "    is_training = tf.placeholder(tf.bool, name='is_training')\n",
    "\n",
    "    # Whenever you need to record the loss, feed the mean test accuracy to this placeholder\n",
    "    with tf.name_scope('acc'):\n",
    "        tf_acc_ph = tf.placeholder(tf.float32,shape=None, name='acc_summary')\n",
    "        # Create a scalar summary object for the accuracy so it can be displayed\n",
    "        tf.summary.scalar('accuracy', tf_acc_ph)\n",
    "\n",
    "    # Use the model function to build the forward pass.\n",
    "    scores = model_init_fn(x, is_training)\n",
    "\n",
    "    # Compute the loss\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=scores)\n",
    "    cross_entropy = tf.reduce_mean(cross_entropy)\n",
    "    loss_reg = tf.losses.get_regularization_loss()\n",
    "    loss = cross_entropy + loss_reg\n",
    "    \n",
    "    tf.summary.scalar('cross_entropy', cross_entropy)\n",
    "    tf.summary.scalar('loss_reg', loss_reg)\n",
    "    tf.summary.scalar('loss', loss)\n",
    "\n",
    "    optimizer = optimizer_init_fn()\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(update_ops):\n",
    "        with tf.name_scope('train'):\n",
    "            train_op = optimizer.minimize(loss)\n",
    "                \n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        #Tensorboard, merge all summaries but the error ones\n",
    "        merged = tf.summary.merge_all(scope=\"(?!acc)\")\n",
    "        merged_acc = tf.summary.merge_all(scope=\"(acc)\")\n",
    "        \n",
    "        log_path = \"C:/tmp/logs\"\n",
    "        train_writer = tf.summary.FileWriter(log_path + '/train/' + experiment_name, sess.graph)\n",
    "        test_writer = tf.summary.FileWriter(log_path + '/test/' + experiment_name)\n",
    "        \n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        saver = tf.train.Saver()\n",
    "        \n",
    "        t = 0\n",
    "        if restore:\n",
    "            saver.restore(sess, f\"C:/tmp/save/{experiment_name}_epoch{epoch_num}.ckpt\")\n",
    "            tqdm.write(f\"Model restored at epoch {epoch_num}.\")\n",
    "            \n",
    "            #update the learning rate the current epoch\n",
    "            if decay_at != None and epoch_num >= decay_at[0]:\n",
    "                idx = np.searchsorted(decay_at, epoch_num, side='right') - 1\n",
    "                optimizer.learning_rate = decay_to[idx]\n",
    "            \n",
    "            #now onto a new epoch\n",
    "            epoch_num += 1\n",
    "            t = len(train_dset) * epoch_num\n",
    "        \n",
    "        for epoch in tnrange(epoch_num, num_epochs+1, leave=False, desc='Epoch'):\n",
    "            #print('\\nStarting epoch %d' % (epoch))\n",
    "            #decay learning rate\n",
    "            if decay_at != None and epoch in decay_at:\n",
    "                optimizer.learning_rate = decay_to[decay_at.index(epoch)]\n",
    "                tqdm.write(f\"Learning rate has changed to: {optimizer.learning_rate}\")\n",
    "            \n",
    "            for x_np, y_np in tqdm_notebook(train_dset, leave=False):\n",
    "                feed_dict = {x: x_np, y: y_np, is_training: True}\n",
    "                summary, loss_np, _ = sess.run([merged, loss, train_op], feed_dict=feed_dict)\n",
    "                train_writer.add_summary(summary, t)\n",
    "                \n",
    "                #check_accuracy and add to tensorboard every 400 steps\n",
    "                if t % 400 == 0 or t % print_every == 0:\n",
    "                    FLAG_print=False\n",
    "                    if t % print_every == 0:\n",
    "                        tqdm.write('Iteration %d, loss = %.4f' % (t, loss_np))\n",
    "                        FLAG_print=True\n",
    "                    acc_test = check_acc_tb(sess, val_dset, x, scores,\n",
    "                                            is_training, FLAG_print=FLAG_print)\n",
    "                    acc_train = check_acc_train(sess, x_np, y_np, x, scores,\n",
    "                                                   is_training, FLAG_print=False)\n",
    "                    test_writer.add_summary(sess.run(merged_acc, feed_dict={tf_acc_ph : acc_test}), t)\n",
    "                    train_writer.add_summary(sess.run(merged_acc, feed_dict={tf_acc_ph : acc_train}), t)\n",
    "                t += 1\n",
    "            #Save every epoch\n",
    "            save_path = saver.save(sess, f\"C:/tmp/save/{experiment_name}_epoch{epoch}.ckpt\")\n",
    "            tqdm.write(\"Model saved in path: %s\" % save_path)\n",
    "                                            \n",
    "        tqdm.write('\\nEnd of training, loss = %.4f' % (loss_np))\n",
    "        acc_test = check_acc_tb(sess, val_dset, x, scores,\n",
    "                                is_training, FLAG_print=FLAG_print)\n",
    "        acc_train = check_acc_train(sess, x_np, y_np, x, scores,\n",
    "                                       is_training, FLAG_print=False)\n",
    "        test_writer.add_summary(sess.run(merged_acc, feed_dict={tf_acc_ph : acc_test}), t)\n",
    "        train_writer.add_summary(sess.run(merged_acc, feed_dict={tf_acc_ph : acc_train}), t)\n",
    "        \n",
    "        #finally test of the testing dataset\n",
    "        acc_test = check_acc_tb(sess, test_dset, x, scores,\n",
    "                                is_training, FLAG_print=False)\n",
    "        tqdm.write(f\"Accuracy on the test dataset is {acc_test}\")\n",
    "        \n",
    "        # Save the variables to disk.\n",
    "        save_path = saver.save(sess, f\"C:/tmp/save/{experiment_name}.ckpt\")\n",
    "        return acc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 182\n",
    "total_layers = 20\n",
    "dropout_prob = 0.5\n",
    "learning_rate = 0.005\n",
    "reg = 2e-4\n",
    "decay_at=[2, 91, 136]\n",
    "decay_to=[0.05, 0.005, 0.0005]\n",
    "print_every = 5000\n",
    "\n",
    "\n",
    "name = (f\"CIFAR_ResNet{total_layers}_lr{learning_rate}\"\n",
    "            f\"_decay{decay_at}_{decay_to}_reg{reg}_momentum\")\n",
    "\n",
    "def model_init_fn(inputs, is_training, total_layers=total_layers, reg=reg):\n",
    "    return model_ResNetv2(inputs, is_training, total_layers=total_layers,\n",
    "                           dropout=dropout_prob, reg=reg)\n",
    "def optimizer_init_fn():\n",
    "    return tf.train.MomentumOptimizer(learning_rate=learning_rate, momentum=0.9)\n",
    "\n",
    "train_part5(model_init_fn, optimizer_init_fn, num_epochs, experiment_name=name, decay_at=decay_at, decay_to=decay_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "total_layers = 20\n",
    "dropout_prob = 0.5\n",
    "learning_rate = 0.001\n",
    "reg = 2e-4\n",
    "decay_at=[91]\n",
    "decay_to=[0.0001]\n",
    "print_every = 1000\n",
    "\n",
    "name = (f\"CIFAR_ResNet{total_layers}_lr{learning_rate}\"\n",
    "            f\"_dp{dropout_prob}_reg{reg}_decay{decay_at}_{decay_to}_adam\")  \n",
    "\n",
    "def model_init_fn(inputs, is_training, total_layers=total_layers, reg=reg):\n",
    "    return model_ResNetv2(inputs, is_training, total_layers=total_layers,\n",
    "                           dropout=dropout_prob, reg=reg)\n",
    "def optimizer_init_fn():\n",
    "    return tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "train_part5(model_init_fn, optimizer_init_fn, num_epochs, experiment_name=name, decay_at=decay_at, decay_to=decay_to)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
